---
layout: single
title: Is AI conscious? Does it matter?
author_profile: true
---

Recently, a pretty prominent, very smart AI researcher posted something on Twitter, and the world is abuzz about it.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">it may be that today&#39;s large neural networks are slightly conscious</p>&mdash; Ilya Sutskever (@ilyasut) <a href="https://twitter.com/ilyasut/status/1491554478243258368?ref_src=twsrc%5Etfw">February 9, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

So, is it true?

First, I think it's probably really hard to define consciousness like, at all. That's a problem for the metaphysicists. But I also think the phrasing here, "slightly conscious", is odd. I've always thought consciousness was a binary proposition, but this brings up a lot of interesting questions for me. What does it mean to be more or less conscious? Is there variation in levels of consciousness among humans in their awake and alert state? In some way, am I more conscious than a person experiencing mental degradation? A baby? Is someone with a higher IQ more- or less-conscious than someone with a lower IQ, or vice-versa? Usually, when we talk about consciousness, we attribute some moral weight to that -- does that extend to variations in consciousness among people, where we might try to add up the levels of consciousness on each path of the tracks in a trolley problem? Lots of interesting questions there!

Anyway, moving on: I think the obvious answer to "Are current AIs conscious" is a resounding "no". But I think the more interesting answer is an even louder "Who cares?"

Let's talk about the biggest neural network that I'm aware of, which is the one that most people associate with spooky apparent consciousness and self-awareness: GPT-3.

GPT-3 is a colossal neural network, a mathematical function with some 175 billion terms. There are plenty of stories of GPT-3 being a little *too* realistic for comfort, like [this guy who convinced GPT-3 to play his dead fiancee](https://www.sfchronicle.com/projects/2021/jessica-simulation-artificial-intelligence/), which is just a hop, skip, and a jump away from a literal Black Mirror episode.

But at the heart of it, GPT-3 is just a mathematical function which, given some text, predicts the most likely word to come next. That's it. It does not have any internal belief or thought or anything at all; it's just a statistical model of how likely any given word is at a specific place in a sentence. You could maybe argue that sentience could arrive from this (or something like it), but that brings me to my next point, which is why we probably don't even need to care if AI is sentient or not.

Human intelligence is a messy, complicated thing that has emerged over the course of eons by an optimization process which is also complicated and messy, but it has a pretty clear goal: more offspring. We tend to think of ourselves as minds inhabiting bodies, but really, we're mostly DNA that has grown a mind to help it make more copies of itself. The things that our minds do are either directly in pursuit of reproduction, or weird side-effects of things that are in pursuit of reproduction, including our survival instinct. Think of the way a dog likes to play; almost all of it simulates chasing, catching, killing, or eating things, because eating is really important to survival and survival is really important to reproducing. This extends to how humans play, too -- lots of games children play involve chasing (or being chased), teamwork, running, throwing, etc. which are skills needed by ancient hunters.

AI is simply, fundamentally not like that. To try to analogize GPT-3 to the games people or dogs like to play, GPT-3 would be deriving pure, unadulterated joy from choosing the correct word that comes next. Maybe, to stretch the analogy a bit, GPT-3 might be in agony if it finds itself in a position where there are no words sufficiently likely. Hopefully, it's obvious that this is a very weak analogy. GPT-3 has no continuity, no internal state, nothing that really persists from one computation to the next.

When I think of what an AI might look like if it were to truly become conscious, in some sense, I think of Mr. Meeseeks.

<iframe src="https://giphy.com/embed/RKYaqZTEHGoqaVATNa" width="480" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/rickandmorty-season-1-adult-swim-rick-and-morty-RKYaqZTEHGoqaVATNa">via GIPHY</a></p>

For the uninitiated, Mr. Meeseeks is a creature (or, a species of creature, all named Mr. Meeseeks) in the adult animated series *Rick and Morty*. When summoned, a Mr. Meeseeks is born into the world in a state of constant agony, and they exist to complete a very specific task. Once the task is complete, they return blissfully to the void of nonexistence. This is, give or take, how I see conscious AI playing out: there is a very specific goal to be accomplished, and the entire purpose of existence for the AI consciousness is to accomplish that goal. There is no reason to want to create art, or yearn for freedom, or find love, or even have any sort of survival instinct at all. Its sole, driving purpose is to accomplish whatever task it was created to accomplish.
